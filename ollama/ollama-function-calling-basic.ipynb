{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4145204a-7d51-47f9-be7d-2fa363bda2ca",
   "metadata": {},
   "source": [
    "# Ollama Function Calling\n",
    "\n",
    "There are 2 mechanism for function calling with Ollama. There is the _traditional_ Ollama method. It can also support the _OpenAI_ function calling API. I am trying both in this instance.\n",
    "\n",
    "First, let's define a dummy function that can get the weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0019776-42dd-4417-b684-a0ae64313bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str)->str:    \n",
    "    '''Get current temperature for a given location.'''\n",
    "    print(f\"Getting weather for: {location}\")\n",
    "    return \"Temperature is 10 degrees\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e372040c-7096-4156-930a-5d9dcb2b978e",
   "metadata": {},
   "source": [
    "Next, let's create a dictionary of available functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e744761f-d5fd-451a-91ef-2d9075332089",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_functions = {\n",
    "    \"get_weather\": get_weather\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014127cf-aa18-4df7-8b9e-ff8ea10ccbd5",
   "metadata": {},
   "source": [
    "## Ollama Method\n",
    "Here is function calling using the traditional Ollama method. Note that the initial call does not actually _invoke_ the function. This has to happen outside of the `ollama.chat` call. So you need to parse the response and then make the function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1ada666-5fef-45b3-85e2-f5a593f51bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'function': {'name': 'get_weather', 'arguments': {'location': 'Toronto'}}}]\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(\n",
    "    model='llama3.2:latest',\n",
    "    messages=[{'role': 'user', 'content':\n",
    "        'What is the weather in Toronto?'}],\n",
    "\n",
    "\t\t# provide a weather checking tool to the model\n",
    "    tools=[{\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'get_weather',\n",
    "        'description': 'Get the current weather for a location',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'location': {\n",
    "              'type': 'string',\n",
    "              'description': 'The name of the city, country. For example, London, United Kingdom.',\n",
    "            },\n",
    "          },\n",
    "          'required': ['location'],\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response['message']['tool_calls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "222c7d0d-6305-4c50-b398-08de82f9a1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for: Toronto\n",
      "Temperature is 10 degrees\n"
     ]
    }
   ],
   "source": [
    "for tool in response[\"message\"][\"tool_calls\"]:\n",
    "    function_to_call = available_functions[tool['function']['name']]\n",
    "    function_args = tool['function']['arguments']\n",
    "    function_response = function_to_call(**function_args)\n",
    "    print(function_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370303da-ef37-4684-b2da-5a9e63f352ef",
   "metadata": {},
   "source": [
    "## OpenAI Method\n",
    "Now let's switch to the OpenAI API. If you want to develop and test your code on local host and then deploy to OpenAI, this is a much better mechanim. Again, the initial call only returns an object for tool calling. You would have to parse the result and actually call the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e5e9201-3ccd-4b99-a365-072da9eadace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='call_g8of7tm5', function=Function(arguments='{\"location\":\"Paris, France\"}', name='get_weather'), type='function', index=0)]\n",
      "Getting weather for: Paris, France\n",
      "Temperature is 10 degrees\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for a given location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and country e.g. London, United Kingdom\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"location\"\n",
    "            ],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama3.2:latest\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}],\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.tool_calls)\n",
    "function_to_call = available_functions[completion.choices[0].message.tool_calls[0].function.name]\n",
    "function_args = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\n",
    "function_response = function_to_call(**function_args)\n",
    "print(function_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
