{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46059d9a-9ff7-4506-ba1a-560d069d8bee",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "The purpose of this notebook is to demonstrate the use of a pre-trained model to build a custom image classifier. It will also show-case the use of data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f566bb6d-d514-478d-bb41-00f6e5f81219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6e479-2505-46a2-a8e2-859cee7baa5a",
   "metadata": {},
   "source": [
    "First, let us define the path. We will be using the following folder hierarchy.\n",
    "\n",
    "```\n",
    "data\n",
    "├── train\n",
    "│   ├── indoor\n",
    "│   └── outdoor\n",
    "└── test\n",
    "    ├── indoor\n",
    "    └── outdoor\n",
    "```\n",
    "\n",
    "I do not have a specific validation folder. Instead, I am using 20% of training images as the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83ef95d0-0404-4245-9fed-3a9f636af560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1. Define Paths\n",
    "# -------------------------\n",
    "base_dir = 'data'  # This folder should contain subfolders 'indoor/' and 'outdoor/'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "#val_dir   = os.path.join(base_dir, 'val')\n",
    "test_dir  = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227a8d0-b89d-4c7c-bfb5-b5f6cdc78340",
   "metadata": {},
   "source": [
    "I am using `ImageDataGenerator` function of `tensorflow` to create variations from the original image. Image will be rotated, stretched, etc. randomly. Note that no actual image is created or stored. This operation happens in-memory while the training process begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41f04ec0-2b08-4ce1-a389-6658dbbe8a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you only have a single dataset folder (with subfolders for each class),\n",
    "# you can split them manually or use ImageDataGenerator's split parameter.\n",
    "\n",
    "# -------------------------\n",
    "# 2. Data Augmentation\n",
    "# -------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    validation_split=0.2  # if using a single folder, 20% for validation    \n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    validation_split=0.2    \n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "461b094a-58e6-4500-9806-5fbb09bca45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 19 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 3. Data Generators\n",
    "# -------------------------\n",
    "batch_size = 8\n",
    "img_size = (224, 224)  # typical for many models like ResNet\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training',  # set to 'training'\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation',  # set to 'validation'\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=1,\n",
    "    class_mode='binary',    \n",
    "    seed=42\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308c00d-d9ca-4677-88fa-d3cbd3068982",
   "metadata": {},
   "source": [
    "In this extremely tiny training set, I have:\n",
    "* Training set = 65 images (around 32 images for each label)\n",
    "* Validation set = 16 images (8 images for each label)\n",
    "* Testing set = 19 images (9 images for each label)\n",
    "This is definitely sub-optimal. However, I want to use this to test my hypothesis. So this is good enough as a first iteration. \n",
    "\n",
    "## Pre-Trained Model\n",
    "For this use-case, I am using the pre-trained model [MobileNetV2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2). In my use case, I have just 50 images each for the 2 labels that I will be training. Due to the small data set, a smaller model like MobileNet would be better suited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32461a8c-5f40-4323-880b-5764f4426006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4. Load a Pretrained Model\n",
    "# -------------------------\n",
    "# We'll use a pretrained MobileNetV2 for speed. You could use ResNet50, VGG16, etc.\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=img_size + (3,),\n",
    "    include_top=False,  # exclude final fully-connected layer\n",
    "    weights='imagenet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f44320fc-2a08-49a6-a528-6453e87dbaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model\n",
    "base_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eddf53d4-140f-48df-9405-570c9713cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 5. Add Custom Layers on Top\n",
    "# -------------------------\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),  # Pools across entire feature map\n",
    "    layers.Dropout(0.2),             # A bit of dropout for regularization\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "481619cd-8f33-4d9b-949b-342ac61526aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff7c870-6dfc-46f9-89a5-256b860dffd6",
   "metadata": {},
   "source": [
    "I added early stopping when I noticed that the model was overfiting after the 7th iteration. However, the early stopping params are not good. The training proceeded as usual. \n",
    "\n",
    "If you notice below, the training should have stopped at iteration 18. Since I have the patience as 3, it hit the 20 epochs limit and training stopped. So I am missing the most optimal point. Maybe I should bump up # of epochs and force the model to stop at iteration # 18.\n",
    "\n",
    "TODO: Fix the early stopping criteria or increase # of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2675a2f7-3131-4095-b967-9cc2ca4376cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an early stopping parameter\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',         # metric to monitor (could also be 'val_accuracy')\n",
    "    patience=3,                 # how many epochs to wait before stopping\n",
    "    restore_best_weights=True   # restore the best model weights at the end\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8ab2834-7d0b-41cf-b9c7-5aa291fe967d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshayranganath/Projects/aiml/venv/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 110ms/step - accuracy: 0.4869 - loss: 0.7842 - val_accuracy: 0.5625 - val_loss: 0.6586\n",
      "Epoch 2/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.6289 - loss: 0.6191 - val_accuracy: 0.6875 - val_loss: 0.5835\n",
      "Epoch 3/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.6966 - loss: 0.5420 - val_accuracy: 0.9375 - val_loss: 0.5361\n",
      "Epoch 4/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.8393 - loss: 0.4380 - val_accuracy: 0.8125 - val_loss: 0.5036\n",
      "Epoch 5/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.8597 - loss: 0.3835 - val_accuracy: 0.9375 - val_loss: 0.4611\n",
      "Epoch 6/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9261 - loss: 0.2980 - val_accuracy: 0.8750 - val_loss: 0.4485\n",
      "Epoch 7/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.9207 - loss: 0.3557 - val_accuracy: 0.8750 - val_loss: 0.4206\n",
      "Epoch 8/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.9348 - loss: 0.2766 - val_accuracy: 0.9375 - val_loss: 0.3841\n",
      "Epoch 9/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.9100 - loss: 0.2909 - val_accuracy: 0.9375 - val_loss: 0.3812\n",
      "Epoch 10/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.8771 - loss: 0.2710 - val_accuracy: 0.7500 - val_loss: 0.4441\n",
      "Epoch 11/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.9192 - loss: 0.2546 - val_accuracy: 0.7500 - val_loss: 0.4290\n",
      "Epoch 12/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9263 - loss: 0.2285 - val_accuracy: 0.9375 - val_loss: 0.3421\n",
      "Epoch 13/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.9693 - loss: 0.1616 - val_accuracy: 1.0000 - val_loss: 0.3275\n",
      "Epoch 14/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.9219 - loss: 0.1699 - val_accuracy: 1.0000 - val_loss: 0.3221\n",
      "Epoch 15/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9628 - loss: 0.1994 - val_accuracy: 0.8750 - val_loss: 0.3234\n",
      "Epoch 16/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9877 - loss: 0.1573 - val_accuracy: 0.8125 - val_loss: 0.3317\n",
      "Epoch 17/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9590 - loss: 0.1838 - val_accuracy: 0.8750 - val_loss: 0.3171\n",
      "Epoch 18/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.1529 - val_accuracy: 0.9375 - val_loss: 0.3002\n",
      "Epoch 19/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9590 - loss: 0.1620 - val_accuracy: 0.8750 - val_loss: 0.3046\n",
      "Epoch 20/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9851 - loss: 0.1231 - val_accuracy: 0.8750 - val_loss: 0.3137\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 6. Train the Model\n",
    "# -------------------------\n",
    "epochs = 20  # Increase if you have more data or can handle more training\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09a5e403-29c6-48ad-973a-fec81f5cc73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 155ms/step - accuracy: 0.6907 - loss: 0.5052 - val_accuracy: 0.9375 - val_loss: 0.2072\n",
      "Epoch 2/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.9553 - loss: 0.2132 - val_accuracy: 0.9375 - val_loss: 0.1573\n",
      "Epoch 3/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 158ms/step - accuracy: 0.8170 - loss: 0.2373 - val_accuracy: 1.0000 - val_loss: 0.1229\n",
      "Epoch 4/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 0.1455 - val_accuracy: 0.9375 - val_loss: 0.1119\n",
      "Epoch 5/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9877 - loss: 0.0665 - val_accuracy: 1.0000 - val_loss: 0.0895\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 7. Fine-Tuning (Optional)\n",
    "# -------------------------\n",
    "# Unfreeze part (or all) of the base model’s layers to fine-tune.\n",
    "\n",
    "# Let's say we unfreeze the last few layers of MobileNetV2:\n",
    "unfreeze_at = 100  # layer index to start unfreezing from\n",
    "for layer in base_model.layers[unfreeze_at:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),  # lower learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "fine_tune_epochs = 5\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=fine_tune_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e97cb400-cc1e-4235-b3b0-a018157122b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.8971 - loss: 0.1664\n",
      "Test accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4472ea-ea75-48c2-9a63-4bc5d7f5e696",
   "metadata": {},
   "source": [
    "This model shows 84% accuracy for the test data set. It is not the greatest but, it is a decent start. So let's save and use it for inference. \n",
    "\n",
    "My main objective - Inference time. If you notice, it is just 37ms. If I had tried to use a visual question answer (VQA) model like LlaVA, this query would have taken a few seconds. Instead, the custom model is orders of magnitude faster. So my hypothesis holds value. It makes sense to invest in this setup when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "985fd0d5-3bc9-45e8-a93b-6d2847728a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model.save(\"indoor_outdoor_classifier_savedmodel.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e8e42-4576-47e4-b262-1f379f3f1aa4",
   "metadata": {},
   "source": [
    "## Inference Pipeline\n",
    "\n",
    "The code below is to build a workflow to run a prediction. This is one-off. For a better inference pipeline, please look at `app.py`. In that file, I am using `streamlit` app to make interactive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f4a8b9d-b065-441e-b4bf-c0c2546e7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to predict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "IMG_SIZE = (224, 224)  # same as in training\n",
    "\n",
    "def predict_single_image(model, img_path, class_names=('indoor', 'outdoor')):\n",
    "    \"\"\"\n",
    "    Loads an image, preprocesses it, and returns the predicted class.\n",
    "    \"\"\"\n",
    "    # 1. Load the image from disk\n",
    "    img = image.load_img(img_path, target_size=IMG_SIZE)\n",
    "\n",
    "    # 2. Convert to array & scale\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = img_array / 255.0  # because we used rescale=1/255 in training\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # model expects batch dimension\n",
    "\n",
    "    # 3. Make prediction\n",
    "    pred = model.predict(img_array)[0][0]\n",
    "    \n",
    "    # 4. Interpret the prediction\n",
    "    #   - If using a single sigmoid output: \n",
    "    #       - p < 0.5 => \"indoor\", p >= 0.5 => \"outdoor\"\n",
    "    #   - Adjust logic if you used a different output layer or threshold\n",
    "    if pred < 0.5:\n",
    "        return class_names[0]  # indoor\n",
    "    else:\n",
    "        return class_names[1]  # outdoor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "742876eb-bf4b-4a28-9306-925499d135ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step\n",
      "Prediction: indoor\n"
     ]
    }
   ],
   "source": [
    "## Loading the model and predicting a single image\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model(\"indoor_outdoor_classifier_savedmodel.keras\")\n",
    "\n",
    "# Predict a single image\n",
    "img_path = \"data/test/pool.jpg\"\n",
    "prediction = predict_single_image(loaded_model, img_path)\n",
    "print(\"Prediction:\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
