{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6b348b-b357-445c-9082-92e446581fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b244b6e9-c932-4f17-8c73-cbadce9e4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=0.1736, std=0.3317)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ce2b81-4611-4c68-a9f0-2294da4c22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.EMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform, split='letters'\n",
    ")\n",
    "test_dataset = torchvision.datasets.EMNIST(\n",
    "    root='./data', train=False, download=True, transform=transform, split='letters'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a3c86e-0ad9-4110-a88d-20da878f16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "# shuffle will prevent the model from memorizing the data\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# we don't want to shuffle since model is done learning\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a90d780c-594e-4e12-8d36-23f754149354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will not use a custom model\n",
    "\n",
    "class EMNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 64, 1, 28, 28 tensor. '64' images '1' is grayscale\n",
    "        # linear vectors wants a single long list of vector\n",
    "        # 28*28 = 784\n",
    "        self.flatten = nn.Flatten() \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 26)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "            x = self.flatten(x) #flatten the data\n",
    "            x = self.layers(x) # pass it through layers\n",
    "            return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c77214-f836-4097-9848-050fe6930bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "source": [
    "# device selection\n",
    "device = torch.device(\"mps\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9208470a-8df5-44ff-b104-5e27c3dcb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model and move it to devic\n",
    "model = EMNISTClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87dd1043-595d-4c12-a9ac-eb5fb33fcee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae5a6ec1-27ff-475f-93bf-bd0bad819d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model,\n",
    "    train_loader,    \n",
    "    loss_function,\n",
    "    optimizer,\n",
    "    device\n",
    "):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # track progress\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1) # what is this doing?\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "            avg_loss = running_loss / 100\n",
    "            accuracy = 100. * correct / total\n",
    "            print(f'\\t[{batch_idx * 64}/ 60000] '\n",
    "                  f'Loss: {avg_loss:.3f} | Accuracy: {accuracy:.1f}%')\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a682a0c-68d5-4898-971c-a97c9f2b7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now evaluate the model\n",
    "def evaluate(\n",
    "    model,\n",
    "    test_loader,\n",
    "    device\n",
    "):\n",
    "    # set to eval mode\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e22b985-61ab-4d68-bd11-b20be9e0d896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "\t[6400/ 60000] Loss: 1.620 | Accuracy: 49.6%\n",
      "\t[12800/ 60000] Loss: 1.050 | Accuracy: 57.3%\n",
      "\t[19200/ 60000] Loss: 0.927 | Accuracy: 60.8%\n",
      "\t[25600/ 60000] Loss: 0.874 | Accuracy: 63.3%\n",
      "\t[32000/ 60000] Loss: 0.780 | Accuracy: 65.2%\n",
      "\t[38400/ 60000] Loss: 0.739 | Accuracy: 66.6%\n",
      "\t[44800/ 60000] Loss: 0.668 | Accuracy: 67.9%\n",
      "\t[51200/ 60000] Loss: 0.620 | Accuracy: 69.0%\n",
      "\t[57600/ 60000] Loss: 0.629 | Accuracy: 69.9%\n",
      "\t[64000/ 60000] Loss: 0.592 | Accuracy: 70.7%\n",
      "\t[70400/ 60000] Loss: 0.547 | Accuracy: 71.5%\n",
      "\t[76800/ 60000] Loss: 0.551 | Accuracy: 72.2%\n",
      "\t[83200/ 60000] Loss: 0.559 | Accuracy: 72.8%\n",
      "\t[89600/ 60000] Loss: 0.523 | Accuracy: 73.3%\n",
      "\t[96000/ 60000] Loss: 0.496 | Accuracy: 73.9%\n",
      "\t[102400/ 60000] Loss: 0.478 | Accuracy: 74.3%\n",
      "\t[108800/ 60000] Loss: 0.502 | Accuracy: 74.7%\n",
      "\t[115200/ 60000] Loss: 0.449 | Accuracy: 75.2%\n",
      "\t[121600/ 60000] Loss: 0.472 | Accuracy: 75.5%\n",
      "Test Accuracy: 83.47%\n",
      "\n",
      "Epoch: 2\n",
      "\t[6400/ 60000] Loss: 0.436 | Accuracy: 82.9%\n",
      "\t[12800/ 60000] Loss: 0.417 | Accuracy: 83.3%\n",
      "\t[19200/ 60000] Loss: 0.428 | Accuracy: 83.1%\n",
      "\t[25600/ 60000] Loss: 0.383 | Accuracy: 83.3%\n",
      "\t[32000/ 60000] Loss: 0.425 | Accuracy: 83.2%\n",
      "\t[38400/ 60000] Loss: 0.404 | Accuracy: 83.2%\n",
      "\t[44800/ 60000] Loss: 0.391 | Accuracy: 83.2%\n",
      "\t[51200/ 60000] Loss: 0.379 | Accuracy: 83.3%\n",
      "\t[57600/ 60000] Loss: 0.413 | Accuracy: 83.4%\n",
      "\t[64000/ 60000] Loss: 0.406 | Accuracy: 83.4%\n",
      "\t[70400/ 60000] Loss: 0.406 | Accuracy: 83.5%\n",
      "\t[76800/ 60000] Loss: 0.377 | Accuracy: 83.5%\n",
      "\t[83200/ 60000] Loss: 0.404 | Accuracy: 83.6%\n",
      "\t[89600/ 60000] Loss: 0.377 | Accuracy: 83.6%\n",
      "\t[96000/ 60000] Loss: 0.376 | Accuracy: 83.7%\n",
      "\t[102400/ 60000] Loss: 0.399 | Accuracy: 83.6%\n",
      "\t[108800/ 60000] Loss: 0.382 | Accuracy: 83.6%\n",
      "\t[115200/ 60000] Loss: 0.398 | Accuracy: 83.7%\n",
      "\t[121600/ 60000] Loss: 0.375 | Accuracy: 83.7%\n",
      "Test Accuracy: 85.03%\n",
      "\n",
      "Epoch: 3\n",
      "\t[6400/ 60000] Loss: 0.338 | Accuracy: 84.8%\n",
      "\t[12800/ 60000] Loss: 0.359 | Accuracy: 84.9%\n",
      "\t[19200/ 60000] Loss: 0.332 | Accuracy: 85.0%\n",
      "\t[25600/ 60000] Loss: 0.358 | Accuracy: 85.0%\n",
      "\t[32000/ 60000] Loss: 0.334 | Accuracy: 85.0%\n",
      "\t[38400/ 60000] Loss: 0.349 | Accuracy: 85.0%\n",
      "\t[44800/ 60000] Loss: 0.309 | Accuracy: 85.1%\n",
      "\t[51200/ 60000] Loss: 0.344 | Accuracy: 85.2%\n",
      "\t[57600/ 60000] Loss: 0.341 | Accuracy: 85.2%\n",
      "\t[64000/ 60000] Loss: 0.331 | Accuracy: 85.2%\n",
      "\t[70400/ 60000] Loss: 0.342 | Accuracy: 85.2%\n",
      "\t[76800/ 60000] Loss: 0.347 | Accuracy: 85.2%\n",
      "\t[83200/ 60000] Loss: 0.350 | Accuracy: 85.2%\n",
      "\t[89600/ 60000] Loss: 0.341 | Accuracy: 85.3%\n",
      "\t[96000/ 60000] Loss: 0.338 | Accuracy: 85.3%\n",
      "\t[102400/ 60000] Loss: 0.345 | Accuracy: 85.3%\n",
      "\t[108800/ 60000] Loss: 0.345 | Accuracy: 85.3%\n",
      "\t[115200/ 60000] Loss: 0.334 | Accuracy: 85.3%\n",
      "\t[121600/ 60000] Loss: 0.351 | Accuracy: 85.3%\n",
      "Test Accuracy: 86.94%\n",
      "\n",
      "Epoch: 4\n",
      "\t[6400/ 60000] Loss: 0.290 | Accuracy: 86.2%\n",
      "\t[12800/ 60000] Loss: 0.296 | Accuracy: 86.2%\n",
      "\t[19200/ 60000] Loss: 0.312 | Accuracy: 86.3%\n",
      "\t[25600/ 60000] Loss: 0.284 | Accuracy: 86.4%\n",
      "\t[32000/ 60000] Loss: 0.307 | Accuracy: 86.5%\n",
      "\t[38400/ 60000] Loss: 0.307 | Accuracy: 86.5%\n",
      "\t[44800/ 60000] Loss: 0.316 | Accuracy: 86.4%\n",
      "\t[51200/ 60000] Loss: 0.314 | Accuracy: 86.4%\n",
      "\t[57600/ 60000] Loss: 0.320 | Accuracy: 86.3%\n",
      "\t[64000/ 60000] Loss: 0.301 | Accuracy: 86.3%\n",
      "\t[70400/ 60000] Loss: 0.316 | Accuracy: 86.3%\n",
      "\t[76800/ 60000] Loss: 0.323 | Accuracy: 86.3%\n",
      "\t[83200/ 60000] Loss: 0.326 | Accuracy: 86.3%\n",
      "\t[89600/ 60000] Loss: 0.323 | Accuracy: 86.3%\n",
      "\t[96000/ 60000] Loss: 0.302 | Accuracy: 86.3%\n",
      "\t[102400/ 60000] Loss: 0.312 | Accuracy: 86.3%\n",
      "\t[108800/ 60000] Loss: 0.321 | Accuracy: 86.2%\n",
      "\t[115200/ 60000] Loss: 0.323 | Accuracy: 86.2%\n",
      "\t[121600/ 60000] Loss: 0.314 | Accuracy: 86.2%\n",
      "Test Accuracy: 87.15%\n",
      "\n",
      "Epoch: 5\n",
      "\t[6400/ 60000] Loss: 0.275 | Accuracy: 86.9%\n",
      "\t[12800/ 60000] Loss: 0.278 | Accuracy: 87.1%\n",
      "\t[19200/ 60000] Loss: 0.300 | Accuracy: 87.1%\n",
      "\t[25600/ 60000] Loss: 0.311 | Accuracy: 86.8%\n",
      "\t[32000/ 60000] Loss: 0.285 | Accuracy: 86.8%\n",
      "\t[38400/ 60000] Loss: 0.292 | Accuracy: 86.7%\n",
      "\t[44800/ 60000] Loss: 0.287 | Accuracy: 86.7%\n",
      "\t[51200/ 60000] Loss: 0.287 | Accuracy: 86.7%\n",
      "\t[57600/ 60000] Loss: 0.299 | Accuracy: 86.7%\n",
      "\t[64000/ 60000] Loss: 0.298 | Accuracy: 86.7%\n",
      "\t[70400/ 60000] Loss: 0.286 | Accuracy: 86.7%\n",
      "\t[76800/ 60000] Loss: 0.282 | Accuracy: 86.7%\n",
      "\t[83200/ 60000] Loss: 0.284 | Accuracy: 86.7%\n",
      "\t[89600/ 60000] Loss: 0.284 | Accuracy: 86.7%\n",
      "\t[96000/ 60000] Loss: 0.294 | Accuracy: 86.7%\n",
      "\t[102400/ 60000] Loss: 0.278 | Accuracy: 86.7%\n",
      "\t[108800/ 60000] Loss: 0.303 | Accuracy: 86.7%\n",
      "\t[115200/ 60000] Loss: 0.318 | Accuracy: 86.7%\n",
      "\t[121600/ 60000] Loss: 0.302 | Accuracy: 86.7%\n",
      "Test Accuracy: 88.14%\n",
      "\n",
      "Epoch: 6\n",
      "\t[6400/ 60000] Loss: 0.247 | Accuracy: 87.7%\n",
      "\t[12800/ 60000] Loss: 0.257 | Accuracy: 87.4%\n",
      "\t[19200/ 60000] Loss: 0.250 | Accuracy: 87.7%\n",
      "\t[25600/ 60000] Loss: 0.258 | Accuracy: 87.6%\n",
      "\t[32000/ 60000] Loss: 0.264 | Accuracy: 87.6%\n",
      "\t[38400/ 60000] Loss: 0.290 | Accuracy: 87.5%\n",
      "\t[44800/ 60000] Loss: 0.280 | Accuracy: 87.4%\n",
      "\t[51200/ 60000] Loss: 0.278 | Accuracy: 87.3%\n",
      "\t[57600/ 60000] Loss: 0.268 | Accuracy: 87.4%\n",
      "\t[64000/ 60000] Loss: 0.275 | Accuracy: 87.3%\n",
      "\t[70400/ 60000] Loss: 0.276 | Accuracy: 87.4%\n",
      "\t[76800/ 60000] Loss: 0.270 | Accuracy: 87.4%\n",
      "\t[83200/ 60000] Loss: 0.295 | Accuracy: 87.3%\n",
      "\t[89600/ 60000] Loss: 0.296 | Accuracy: 87.2%\n",
      "\t[96000/ 60000] Loss: 0.295 | Accuracy: 87.2%\n",
      "\t[102400/ 60000] Loss: 0.282 | Accuracy: 87.2%\n",
      "\t[108800/ 60000] Loss: 0.295 | Accuracy: 87.2%\n",
      "\t[115200/ 60000] Loss: 0.270 | Accuracy: 87.2%\n",
      "\t[121600/ 60000] Loss: 0.294 | Accuracy: 87.2%\n",
      "Test Accuracy: 88.21%\n",
      "\n",
      "Epoch: 7\n",
      "\t[6400/ 60000] Loss: 0.261 | Accuracy: 87.4%\n",
      "\t[12800/ 60000] Loss: 0.231 | Accuracy: 87.8%\n",
      "\t[19200/ 60000] Loss: 0.263 | Accuracy: 87.9%\n",
      "\t[25600/ 60000] Loss: 0.269 | Accuracy: 87.7%\n",
      "\t[32000/ 60000] Loss: 0.252 | Accuracy: 87.7%\n",
      "\t[38400/ 60000] Loss: 0.251 | Accuracy: 87.7%\n",
      "\t[44800/ 60000] Loss: 0.262 | Accuracy: 87.7%\n",
      "\t[51200/ 60000] Loss: 0.262 | Accuracy: 87.7%\n",
      "\t[57600/ 60000] Loss: 0.263 | Accuracy: 87.6%\n",
      "\t[64000/ 60000] Loss: 0.255 | Accuracy: 87.6%\n",
      "\t[70400/ 60000] Loss: 0.270 | Accuracy: 87.6%\n",
      "\t[76800/ 60000] Loss: 0.254 | Accuracy: 87.5%\n",
      "\t[83200/ 60000] Loss: 0.258 | Accuracy: 87.5%\n",
      "\t[89600/ 60000] Loss: 0.265 | Accuracy: 87.5%\n",
      "\t[96000/ 60000] Loss: 0.273 | Accuracy: 87.5%\n",
      "\t[102400/ 60000] Loss: 0.296 | Accuracy: 87.4%\n",
      "\t[108800/ 60000] Loss: 0.269 | Accuracy: 87.4%\n",
      "\t[115200/ 60000] Loss: 0.282 | Accuracy: 87.4%\n",
      "\t[121600/ 60000] Loss: 0.259 | Accuracy: 87.4%\n",
      "Test Accuracy: 88.67%\n",
      "\n",
      "Epoch: 8\n",
      "\t[6400/ 60000] Loss: 0.217 | Accuracy: 88.9%\n",
      "\t[12800/ 60000] Loss: 0.234 | Accuracy: 88.7%\n",
      "\t[19200/ 60000] Loss: 0.241 | Accuracy: 88.6%\n",
      "\t[25600/ 60000] Loss: 0.243 | Accuracy: 88.4%\n",
      "\t[32000/ 60000] Loss: 0.244 | Accuracy: 88.4%\n",
      "\t[38400/ 60000] Loss: 0.255 | Accuracy: 88.4%\n",
      "\t[44800/ 60000] Loss: 0.267 | Accuracy: 88.2%\n",
      "\t[51200/ 60000] Loss: 0.242 | Accuracy: 88.1%\n",
      "\t[57600/ 60000] Loss: 0.247 | Accuracy: 88.1%\n",
      "\t[64000/ 60000] Loss: 0.243 | Accuracy: 88.1%\n",
      "\t[70400/ 60000] Loss: 0.256 | Accuracy: 88.1%\n",
      "\t[76800/ 60000] Loss: 0.262 | Accuracy: 88.0%\n",
      "\t[83200/ 60000] Loss: 0.266 | Accuracy: 87.9%\n",
      "\t[89600/ 60000] Loss: 0.276 | Accuracy: 87.9%\n",
      "\t[96000/ 60000] Loss: 0.259 | Accuracy: 87.8%\n",
      "\t[102400/ 60000] Loss: 0.262 | Accuracy: 87.8%\n",
      "\t[108800/ 60000] Loss: 0.267 | Accuracy: 87.8%\n",
      "\t[115200/ 60000] Loss: 0.263 | Accuracy: 87.8%\n",
      "\t[121600/ 60000] Loss: 0.260 | Accuracy: 87.8%\n",
      "Test Accuracy: 88.87%\n",
      "\n",
      "Epoch: 9\n",
      "\t[6400/ 60000] Loss: 0.230 | Accuracy: 88.3%\n",
      "\t[12800/ 60000] Loss: 0.236 | Accuracy: 88.3%\n",
      "\t[19200/ 60000] Loss: 0.246 | Accuracy: 88.1%\n",
      "\t[25600/ 60000] Loss: 0.235 | Accuracy: 88.2%\n",
      "\t[32000/ 60000] Loss: 0.235 | Accuracy: 88.3%\n",
      "\t[38400/ 60000] Loss: 0.260 | Accuracy: 88.2%\n",
      "\t[44800/ 60000] Loss: 0.230 | Accuracy: 88.3%\n",
      "\t[51200/ 60000] Loss: 0.245 | Accuracy: 88.3%\n",
      "\t[57600/ 60000] Loss: 0.215 | Accuracy: 88.3%\n",
      "\t[64000/ 60000] Loss: 0.238 | Accuracy: 88.3%\n",
      "\t[70400/ 60000] Loss: 0.251 | Accuracy: 88.2%\n",
      "\t[76800/ 60000] Loss: 0.249 | Accuracy: 88.2%\n",
      "\t[83200/ 60000] Loss: 0.251 | Accuracy: 88.2%\n",
      "\t[89600/ 60000] Loss: 0.242 | Accuracy: 88.2%\n",
      "\t[96000/ 60000] Loss: 0.240 | Accuracy: 88.2%\n",
      "\t[102400/ 60000] Loss: 0.251 | Accuracy: 88.1%\n",
      "\t[108800/ 60000] Loss: 0.257 | Accuracy: 88.1%\n",
      "\t[115200/ 60000] Loss: 0.251 | Accuracy: 88.1%\n",
      "\t[121600/ 60000] Loss: 0.248 | Accuracy: 88.1%\n",
      "Test Accuracy: 88.81%\n",
      "\n",
      "Epoch: 10\n",
      "\t[6400/ 60000] Loss: 0.209 | Accuracy: 89.4%\n",
      "\t[12800/ 60000] Loss: 0.223 | Accuracy: 89.1%\n",
      "\t[19200/ 60000] Loss: 0.208 | Accuracy: 89.1%\n",
      "\t[25600/ 60000] Loss: 0.222 | Accuracy: 89.0%\n",
      "\t[32000/ 60000] Loss: 0.236 | Accuracy: 88.9%\n",
      "\t[38400/ 60000] Loss: 0.239 | Accuracy: 88.8%\n",
      "\t[44800/ 60000] Loss: 0.252 | Accuracy: 88.6%\n",
      "\t[51200/ 60000] Loss: 0.222 | Accuracy: 88.6%\n",
      "\t[57600/ 60000] Loss: 0.226 | Accuracy: 88.6%\n",
      "\t[64000/ 60000] Loss: 0.267 | Accuracy: 88.5%\n",
      "\t[70400/ 60000] Loss: 0.247 | Accuracy: 88.4%\n",
      "\t[76800/ 60000] Loss: 0.219 | Accuracy: 88.4%\n",
      "\t[83200/ 60000] Loss: 0.251 | Accuracy: 88.3%\n",
      "\t[89600/ 60000] Loss: 0.243 | Accuracy: 88.3%\n",
      "\t[96000/ 60000] Loss: 0.242 | Accuracy: 88.3%\n",
      "\t[102400/ 60000] Loss: 0.237 | Accuracy: 88.3%\n",
      "\t[108800/ 60000] Loss: 0.245 | Accuracy: 88.3%\n",
      "\t[115200/ 60000] Loss: 0.238 | Accuracy: 88.3%\n",
      "\t[121600/ 60000] Loss: 0.245 | Accuracy: 88.3%\n",
      "Test Accuracy: 88.76%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch: {epoch + 1}')\n",
    "    train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        device    \n",
    "    )\n",
    "    accuracy = evaluate(\n",
    "        model,\n",
    "        train_loader,\n",
    "        device\n",
    "    )\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43847307-c2e3-407b-b3d8-042f953ecbb2",
   "metadata": {},
   "source": [
    "## How to see what's inside the mode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d427aa66-75b4-48e8-af9a-7796705b1584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x10f1c25e0>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee7914d8-2783-4956-ba23-610b4f54040d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784])\n",
      "torch.Size([128])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a88bb7b7-9563-43f6-9a97-87103e0c39dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 103834\n"
     ]
    }
   ],
   "source": [
    "# how to get the total parameters\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14e40690-13d7-4672-87ae-de75f59c42a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight: torch.Size([128, 784])\n",
      "layers.0.bias: torch.Size([128])\n",
      "layers.2.weight: torch.Size([26, 128])\n",
      "layers.2.bias: torch.Size([26])\n"
     ]
    }
   ],
   "source": [
    "# named parameters to understand how each layer looks like.\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8434d0d7-5e79-4b37-96c2-8a90ea007efd",
   "metadata": {},
   "source": [
    "PyTorch allows for `children` and `modules` for the parameters. Think of the model of a folder structure. `children` will just show the first level. `module` will show everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1b05c-604b-4ebb-8f11-9ee82fb16ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
